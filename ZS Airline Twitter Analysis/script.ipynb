{"cells":[{"metadata":{"_uuid":"433c94a7173ab62e7b630eef72206ba21d31a6ec"},"cell_type":"markdown","source":"# Importing Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport nltk #for natural language processing\nimport string\nfrom PIL import Image\nfrom sklearn.feature_extraction.text import TfidfVectorizer,TfidfTransformer\n\nfrom nltk.corpus import stopwords\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport emoji\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport re\nfrom nltk.stem import PorterStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport collections\nfrom sklearn.model_selection import train_test_split\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d0ac31e53ae0219c071530dbb9f3838dd8c8fb44"},"cell_type":"markdown","source":"# Importing Files"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df=pd.read_csv(\"../input/train.csv\")\ntest_df=pd.read_csv(\"../input/test.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1748321bf7d3de37527dc53e4df8ad39e29604c"},"cell_type":"markdown","source":"# General Analysis"},{"metadata":{"trusted":true,"_uuid":"f9fbd8b50638f0d1c1c41d02c67ad482ef3089aa"},"cell_type":"code","source":"#To check first five rows\ntrain_df.head()\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"22587afb819613fb7abe0385402e3f73b034dfc1"},"cell_type":"code","source":"#To check type of columns\ntrain_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8e7a5dc51fb72accbdbedf25d87a511e3495e31"},"cell_type":"code","source":"#To check size of data\nprint(train_df.shape)\nprint(\"training set has 3339 rows and 12 columns\\n \")\n#To check no. of null values\nprint(train_df.isnull().sum())\nprint(\"\\nFollowing columns have null values more than 1:-\\r\\n1. negativereason\\r\\n2. negativereason_confidence\\r\\n3. tweet_created\\r\\n4. tweet_location\\r\\n5. usertimezone \")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e95e6fdeaa0c183f3f1f7d7950d617bc882eaa0e"},"cell_type":"markdown","source":"* The main objective here is to determine whether the  tweet is negative or positive .\n* airline_sentiment column shows the type of tweet : Negative, Positive or Neutral\n* negativereason column shows the overall reason for a negative tweet. Its not applicable for Positive or Neutral tweets\n* airline column shows the name of the airline as a particular airline may have more negative tweets than other . We will look into that.\n* text column is the actual tweet which contains words deciding whether the tweet is negative or positive.\n* Rest of the columns have not been used in this notebook as of now.\n"},{"metadata":{"trusted":true,"_uuid":"101373cf4e20dba44b651f84a3b5256bc217ef95"},"cell_type":"code","source":"#To count the no. of negative , positive and neutral tweets in training data\nmood_count=train_df[\"airline_sentiment\"].value_counts()\nmood_count","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"edc8308f5d2ad206d11e98582a4685cca3b6ca44"},"cell_type":"markdown","source":"* Most of the tweets are negative which makes sense as people tweet mostly when they had some issues with the flight."},{"metadata":{"trusted":true,"_uuid":"493770d1d23e4d827cc260dfc41999966d436e3f"},"cell_type":"code","source":"# To plot the abouve stats\nplt.bar([\"Negative\",\"Neutral\",'Positive'],mood_count)\nplt.xlabel(\"Mood\")\nplt.ylabel(\"Mood_Count\")\nplt.xticks(rotation=45)\nplt.title(\"Count of Moods\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56328e83471c3997e177cb3d1a78aeb6e6fc54ae"},"cell_type":"code","source":"# To find the count of tweets for different airlines\ntrain_df[\"airline\"].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af7ab7b2d2858b16f01055cc883623fe81065e59"},"cell_type":"markdown","source":"* United airlines have most no. of tweets."},{"metadata":{"trusted":true,"_uuid":"19c1a2852fd751e9678bc468b601d6239abf1c70"},"cell_type":"code","source":"# To plot the sentiment count airline wise\ndef plot_airline_wise_sentiments(Airline):\n    df=train_df[train_df[\"airline\"]==Airline]\n    count=df[\"airline_sentiment\"].value_counts()\n    plt.bar([\"Negative\",\"Neutral\",\"Positive\"],count)\n    plt.xlabel(\"Moods\")\n    plt.ylabel(\"Moood_Counts\")\n    plt.title(\"Mood counts for {}\".format(Airline))\nplt.figure(1,figsize=(20,12))\nplt.subplot(231)\nplot_airline_wise_sentiments(\"United\")\nplt.subplot(232)\nplot_airline_wise_sentiments(\"Virgin America\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3832193c0eed34bc3c4e9d8ccfa67d67d120d6fd"},"cell_type":"markdown","source":"* From Graphs , its clear that United airlines has much more negative tweets and almost same no. of neutral and positive tweets while for Virgin      America, sentiments are somewhat balanced"},{"metadata":{"trusted":true,"_uuid":"7d96b50660c24df100d759e60e1b713b5e47fb08","scrolled":true},"cell_type":"code","source":"# To get the count of negative reasons . Here dict function is used to create dictionary.\nNR_count=dict(train_df[\"negativereason\"].value_counts())\nprint(NR_count)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f0753662ce64e41e51420ed7bd7e2072ffb4d5e","collapsed":true},"cell_type":"code","source":"# To get airline wise count of negative reasons\ndef NR_count(Airline):\n    if(Airline==\"All\"):\n        df=train_df\n    else:\n        df=train_df[train_df[\"airline\"]==Airline]\n    count=dict(df[\"negativereason\"].value_counts())\n    unique_reason=list(train_df[\"negativereason\"].unique())\n    unique_reason=[x for x in unique_reason if str(x)!='nan'] # To remove none values\n    print(type(unique_reason))\n    reason_frame=pd.DataFrame({'Reasons':unique_reason})\n    reason_frame['Count']=reason_frame['Reasons'].apply(lambda x:count[x])\n    return reason_frame\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2b816cafbe99f56ade1b49d153fe97372e53601"},"cell_type":"code","source":"# To plot airline wise count of negative reason\ndef plot_reason(Airline):\n    df=NR_count(Airline)\n    index=df[\"Reasons\"]\n    plt.figure(figsize=(12,12))\n    plt.bar(index,df[\"Count\"])\n    plt.xticks(rotation=45)\n    plt.tick_params(top='off', bottom='on', left='off', right='off', labelleft='on', labelbottom='on')\n    plt.xlabel(\"Negative Reasons\")\n    plt.ylabel(\"Count\")\n    plt.title(\"Negative reason count for \"+Airline)\n\nplot_reason(\"All\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5963c246ee81e761262e2619b95f1d3feb96c774"},"cell_type":"code","source":"plot_reason(\"United\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eefe1699beb95a604d2a1e75f32707699f6e0956"},"cell_type":"code","source":"plot_reason(\"Virgin America\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e95052835c0a82b9ad08d747dfd576b8a230b1e"},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{"trusted":true,"_uuid":"ddb6f7fda64c5be459f5cdbe4f8fa44e4416cb78","collapsed":true},"cell_type":"code","source":"# Functions to remove unnecesary words, symbols from text .\ndef remove_mentions(input_text): # To remove @....\n    return re.sub(r'@\\w+','',str(input_text))\ndef remove_urls(input_text): # To remove http.......\n    return re.sub(r'http.?://[^\\s]+[\\s]?', '', str(input_text))\ndef emoji_oneword(input_text): # To remove emojis\n    return input_text.replace('_','')\ndef remove_punctuation(input_text): # To remove punctuations(, . ! ') \n    punct=string.punctuation # punct now has all the punctuations used in english \n    trantab=str.maketrans(punct,len(punct)*' ') #every punctuation in punct will be mapped to ' ' and stored in trantab in a table\n    return  input_text.translate(trantab) # Here punctuations in text will be replaced by ' ' as defined in trantab.\ndef remove_digits(input_text): # To remove digits \n     return re.sub(r'\\d+','',str(input_text))\ndef to_lower(input_text): # To convert each word in lower case\n     return input_text.lower()\ndef remove_stopWords(input_text): # To remove stop words like the, is , in ,not.\n    stopwords_list = stopwords.words('english')\n    whitelist=[\"n't\",\"no\",\"not\"] # Some words which might indicate a certain sentiment are kept via a whitelist\n    words=input_text.split() # By default it will split the words by ' '\n    clean_words=[word for word in words if(word not in stopwords_list or word in whitelist) and len(word)>1]\n    return \" \".join(clean_words)\ndef stemming(input_text): # stemming means getting word to its original form eg: Difficulty -> Difficult\n    porter=PorterStemmer()\n    words=input_text.split()\n    stemmed_words = [porter.stem(word) for word in words]\n    return \" \".join(stemmed_words)\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"884f48e1d69084120871ee88c078209327a9d392","collapsed":true},"cell_type":"code","source":"pd.options.mode.chained_assignment = None  # default='warn' # To hide warnings\ndf=train_df[train_df[\"airline_sentiment\"]==\"negative\"]\ndf[\"text\"]=df[\"text\"].fillna(\"No Value Found\")\ndf[\"text\"]=df[\"text\"].apply(lambda x: emoji.demojize(x)) #To covert emoji in txt\ndf[\"text\"]=df[\"text\"].apply(lambda x: remove_mentions(x))\ndf[\"text\"]=df[\"text\"].apply(lambda x: remove_urls(x))\ndf[\"text\"]=df[\"text\"].apply(lambda x: emoji_oneword(x))\ndf[\"text\"]=df[\"text\"].apply(lambda x: remove_punctuation(x))\ndf[\"text\"]=df[\"text\"].apply(lambda x: remove_digits(x))\ndf[\"text\"]=df[\"text\"].apply(lambda x: to_lower(x))\ndf[\"text\"]=df[\"text\"].apply(lambda x: remove_stopWords(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40d0cdb994c9ec13bde67d0ed1eb2b4d19d5aad3","collapsed":true},"cell_type":"code","source":"# To create Word Cloud of most frequent negative words\nwords=' '.join( x for x in str(df.text.values).split())             \nfrom wordcloud import WordCloud, STOPWORDS,ImageColorGenerator\nd = os.path.dirname(\"../input/\")\nplane_coloring = np.array(Image.open(os.path.join(d, \"Airplane_Transparent_PNG_Clipart.png\")))\n\nwordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='white',\n                      width=3000,\n                      height=2500,\n                      colormap=\"Blues\",\n                      max_words=15,\n                      mask=plane_coloring\n                     ).generate(words)\nimage_colors = ImageColorGenerator(plane_coloring)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa998f362cd6a2b02317247b0bc73e52d4fef80f"},"cell_type":"code","source":"plt.figure(1,figsize=(25,25))\nplt.imshow(wordcloud,interpolation=\"bilinear\")\nplt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\")\n#plt.imshow(plane_coloring, cmap=plt.cm.gray, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4a3d30fe2c7881aaeca1047318c2416088099654"},"cell_type":"code","source":"# Function to get meaningful words from training file\ndef tweet_to_words(raw_tweet):\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \",str(raw_tweet)) \n    words = letters_only.lower().split()                             \n    stops = set(stopwords.words(\"english\"))                  \n    meaningful_words = [w for w in words if not w in stops] \n    return( \" \".join( meaningful_words ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a4792eab4f3d822188912a956527c74fd12ad4e0"},"cell_type":"code","source":"# Function to get length of meaningful words from training file\ndef clean_tweet_length(raw_tweet):\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \",str(raw_tweet))\n    words = letters_only.lower().split()                             \n    stops = set(stopwords.words(\"english\"))                  \n    meaningful_words = [w for w in words if not w in stops] \n    return(len(meaningful_words))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1fb2c97849a7105983fd6d22cc3e5fe668f28776","collapsed":true},"cell_type":"code","source":"# Changing the sentiment column values in numerical categorical form as we will only predict whether the sentiment is positive or negative, considering neutral sentiments as positive\ntrain_df['sentiment']=train_df['airline_sentiment'].apply(lambda x: -1 if x=='negative' else(0 if x=='neutral' else 1))\n#test_df['sentiment']=test_df['airline_sentiment'].apply(lambda x: -1 if x=='negative' else(0 if x=='neutral' else 1))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"caa1009d06efc2b466683e959c7f3cf601abc3c2","collapsed":true},"cell_type":"code","source":"#train_df['clean_tweet']=train_df['text'].apply(lambda x: tweet_to_words(x))\ntrain_df[\"text\"]=train_df[\"text\"].fillna(\"No Value Found\")\ntrain_df[\"text\"]=train_df[\"text\"].apply(lambda x: emoji.demojize(x)) #To covert emoji in txt\ntrain_df[\"text\"]=train_df[\"text\"].apply(lambda x: remove_mentions(x))\ntrain_df[\"text\"]=train_df[\"text\"].apply(lambda x: remove_urls(x))\ntrain_df[\"text\"]=train_df[\"text\"].apply(lambda x: emoji_oneword(x))\ntrain_df[\"text\"]=train_df[\"text\"].apply(lambda x: remove_punctuation(x))\ntrain_df[\"text\"]=train_df[\"text\"].apply(lambda x: remove_digits(x))\ntrain_df[\"text\"]=train_df[\"text\"].apply(lambda x: to_lower(x))\ntrain_df[\"text\"]=train_df[\"text\"].apply(lambda x: remove_stopWords(x))\ntrain_df['clean_tweet']=train_df['text']\ntrain_df['Tweet_length']=train_df['text'].apply(lambda x: clean_tweet_length(x))\ntrain,test = train_test_split(train_df,test_size=0.2,random_state=42)\n# for original test data to be used later\n#test_df['clean_tweet']=test_df['text'].apply(lambda x: tweet_to_words(x))\ntest_df[\"text\"]=test_df[\"text\"].fillna(\"No Value Found\")\ntest_df[\"text\"]=test_df[\"text\"].apply(lambda x: emoji.demojize(x)) #To covert emoji in txt\ntest_df[\"text\"]=test_df[\"text\"].apply(lambda x: remove_mentions(x))\ntest_df[\"text\"]=test_df[\"text\"].apply(lambda x: remove_urls(x))\ntest_df[\"text\"]=test_df[\"text\"].apply(lambda x: emoji_oneword(x))\ntest_df[\"text\"]=test_df[\"text\"].apply(lambda x: remove_punctuation(x))\ntest_df[\"text\"]=test_df[\"text\"].apply(lambda x: remove_digits(x))\ntest_df[\"text\"]=test_df[\"text\"].apply(lambda x: to_lower(x))\ntest_df[\"text\"]=test_df[\"text\"].apply(lambda x: remove_stopWords(x))\ntest_df['clean_tweet']=test_df['text']\ntest_df['Tweet_length']=test_df['text'].apply(lambda x: clean_tweet_length(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9c2b4d521bc42fac62474396c141a545f7f43896"},"cell_type":"code","source":"# Creating train and test clean tweet words data\ntrain_clean_tweet=[]\nfor tweet in train['clean_tweet']:\n    train_clean_tweet.append(tweet)\ntest_clean_tweet=[]\nfor tweet in test['clean_tweet']:\n    test_clean_tweet.append(tweet)\n    \n# for original train and test data to be used later\ntrain_original_clean_tweet=[]\nfor tweet in train_df['clean_tweet']:\n    train_original_clean_tweet.append(tweet)\ntest_original_clean_tweet=[]\nfor tweet in test_df['clean_tweet']:\n    test_original_clean_tweet.append(tweet)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6efa48d72066871d20502035b51db2034056cc98"},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n#v = CountVectorizer(analyzer = \"word\")\nv = TfidfVectorizer(analyzer=\"word\")\ntrain_features= v.fit_transform(train_clean_tweet)\n#print(train_features)\nword_freq = dict(zip(v.get_feature_names(), np.asarray(train_features.sum(axis=0)).ravel())) #zip function is used for mapping values in different lists\n#print(train_features.sum(axis=0))\n#print(word_freq)\nword_counter = collections.Counter(word_freq)\n#print(word_counter)\nword_counter_df = pd.DataFrame(word_counter.most_common(20), columns = ['word', 'freq'])\nfig, ax = plt.subplots(figsize=(12, 10))\nsns.barplot(x=\"word\", y=\"freq\", data=word_counter_df, palette=\"PuBuGn_d\", ax=ax)\nplt.xticks(rotation=45)\nplt.show();\ntest_features=v.transform(test_clean_tweet)\n\n# for original train and test data to be used later\ntrain_original_features= v.fit_transform(train_original_clean_tweet)\ntest_original_features=v.transform(test_original_clean_tweet)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"04b90fdf3d82fc7a9c877a4a8fc5592d7b1bf6a0"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6a09455c8832961a03de8625dbf9e55dcff07bdf"},"cell_type":"code","source":"Classifiers = [\n    LogisticRegression(C=0.000000001,solver='liblinear',max_iter=200),\n    KNeighborsClassifier(3),\n    SVC(kernel=\"rbf\", C=0.025, probability=True,gamma='auto'),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(n_estimators=200),\n    AdaBoostClassifier(),\n    GaussianNB()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d583d9acce2c6e24d909f02cd9634d6d4d71e98"},"cell_type":"code","source":"dense_features=train_features.toarray()\ndense_test= test_features.toarray()\nAccuracy=[]\nModel=[]\nfor classifier in Classifiers:\n    try:\n        fit = classifier.fit(train_features,train['sentiment'])\n        pred = fit.predict(test_features)\n    except Exception:\n        fit = classifier.fit(dense_features,train['sentiment'])\n        pred = fit.predict(dense_test)\n    \n    accuracy = accuracy_score(pred,test['sentiment'])\n    Accuracy.append(accuracy)\n    Model.append(classifier.__class__.__name__)\n    print('Accuracy of '+classifier.__class__.__name__+' is '+str(accuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6fa993aefee9cd1a2578f74c4581afd1f92125e9"},"cell_type":"code","source":"Index = [1,2,3,4,5,6,7]\nplt.bar(Index,Accuracy)\nplt.xticks(Index, Model,rotation=45)\nplt.ylabel('Accuracy')\nplt.xlabel('Model')\nplt.title('Accuracies of Models')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df080c1ae07febca5d4b145d1d10d1f1dc58305f"},"cell_type":"markdown","source":"* Using the best model"},{"metadata":{"trusted":true,"_uuid":"a7510f120e8f0eff846932684ac26009a85c571a","collapsed":true},"cell_type":"code","source":"dense_original_features=train_original_features.toarray()\ndense_original_test= test_original_features.toarray()\nindex=Accuracy.index(max(Accuracy))\nclassifier=Classifiers[index]\ntry:\n    fit = classifier.fit(train_original_features,train_df['sentiment'])\n    pred = fit.predict(test_original_features)\nexcept Exception:\n    fit = classifier.fit(dense_original_features,train_df['sentiment'])\n    pred = fit.predict(dense_original_test)\npred=pred.astype(object)\npred[pred==0]='neutral'\npred[pred==-1]='negative'\npred[pred==1]='positive'\nd={\"tweet_id\":test_df.tweet_id,\"airline_sentiment\":pred}\nsubmission=pd.DataFrame(d)\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9259117ae1bde5a29b0f254458e381953e1ec020"},"cell_type":"markdown","source":" # References\n \n ## Text Analytics Reference\n* http://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/\n* https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n* https://www.kaggle.com/bertcarremans/predicting-sentiment-with-text-features\n* https://www.kaggle.com/adamschroeder/countvectorizer-tfidfvectorizer-predict-comments\n\n## Predictive Model Reference\n* https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/  (KNN Algorithm)\n* https://www.analyticsvidhya.com/blog/2015/05/boosting-algorithms-simplified/ (AdaBoost)\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5e3374e741d346b571b465de7513a17c613fc2d9"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}